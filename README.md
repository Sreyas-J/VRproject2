# Introduction
Multimodal Visual Question Answering (VQA) is a challenging research problem that lies at the intersection of computer vision and natural language processing. It involves building models capable of answering questions based on image inputs, demanding an understanding of both visual semantics and linguistic context. With increasing demand for AI systems that can interpret and reason over multimodal data, VQA has found applications in accessibility tools, educational technologies, and intelligent virtual assistants.

This project explores multimodal VQA using the Amazon Berkeley Objects (ABO) dataset, which consists of over 147,000 product listings and nearly 400,000 catalog images with multilingual metadata. The primary objective of this mini-project is to curate a diverse single-word answer VQA dataset using this multimodal data, evaluate pre-trained baseline models on it, and enhance performance through parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA). 
# Important Contents
```bash
- vrproject2baselineblip.ipynb - BLIP baseline
- vrproject2baselinevilt.ipynb - ViLT baseline
- data_curation.ipynb - Performs preprocessing on the ABO dataset to filter and select appropriate images for generating the VQA dataset
- vqa_generator.ipynb - Generates visual question-answer pairs using the curated images and the Gemini API to create a single-word answer VQA dataset
- vqa_dataset.csv - The dataset generated by vqa_generator, used to fine-tune
- fine-tune-model.ipynb - Fine-tunes the BLIP model using the generated VQA dataset and evaluates its performance using standard metrics
- inference-setup - Implementation files
- model/kaggle/working - Contains kaggle files of the model
```
# Inference Instructions

Follow the steps below to run inference on the curated dataset:
## 0. Create Conda Environment (Python 3.9)
Create and activate a Conda environment with Python 3.9:
```bash
conda create -n vqa_env python=3.9
```
```bash
conda activate vr-eval
```

## 1. Setup


- For **Linux**:
  ```bash
  cd inference-setup
  ```
## 2. Move to the sample-submission directory:
  ```bash
  cd sample-submission
  ```

## 3. Install Dependencies
```bash
python -m pip install -r requirements.txt
```

## 4. Run Inference Script
Navigate back a folder and run the inference script
``` bash
cd ..
```
```bash
python run_inference_for_all_public.py
```

