{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11818065,"sourceType":"datasetVersion","datasetId":7315704},{"sourceId":11851151,"sourceType":"datasetVersion","datasetId":7361448}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --no-deps bert-score\n!pip install --quiet bitsandbytes\n!pip install evaluate\n!pip install nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Imports and Initial Setup**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    BlipProcessor,\n    BlipForQuestionAnswering,\n    default_data_collator,\n    TrainingArguments,\n    Trainer,\n    BertTokenizer,\n    BertModel\n)\nfrom peft import (\n    prepare_model_for_kbit_training,\n    LoraConfig,\n    get_peft_model,\n    PeftModel,\n)\nimport wandb\nimport types\nimport re\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom bert_score import score as bert_score\nimport nltk\nfrom nltk.corpus import wordnet as wn\nfrom torch.utils.data import IterableDataset\n\n# Download required NLTK resources\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('averaged_perceptron_tagger_eng')\n\n# Set device to GPU if available, otherwise CPU\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Patching BLIP Models**\n* **Problem**: The BLIP model's forward methods assume inputs like input_ids (token IDs for text) or attention_mask have a batch dimension. For example, they expect input_ids to be shaped like [batch_size, sequence_length]. If you pass a single sample with shape [sequence_length], the model will fail because it can't process the unexpected shape.\n* **Solution**: The patch checks if the input (input_ids, attention_mask, or labels) is missing the batch dimension (i.e., has only one dimension). If so, it adds a batch dimension by using unsqueeze(0), transforming the shape from [sequence_length] to [1, sequence_length]. Then, it calls the original forward method with the corrected inputs","metadata":{}},{"cell_type":"code","source":"def patch_blip_models():\n    # Import BLIP text model classes for modification\n    from transformers.models.blip.modeling_blip_text import BlipTextModel, BlipTextLMHeadModel\n    \n    # Store original forward method of BlipTextModel for patching\n    original_text_forward = BlipTextModel.forward\n    def patched_text_forward(self, input_ids=None, attention_mask=None, position_ids=None, \n                             head_mask=None, inputs_embeds=None, encoder_embeds=None,\n                             encoder_hidden_states=None, encoder_attention_mask=None, \n                             past_key_values=None, use_cache=None, output_attentions=None,\n                             output_hidden_states=None, return_dict=None, is_decoder=False):\n        # Check if input_ids lacks batch dimension (i.e., is 1D)\n        if input_ids is not None and len(input_ids.shape) == 1:\n            # Add batch dimension to input_ids (e.g., [seq_len] -> [1, seq_len])\n            input_ids = input_ids.unsqueeze(0)\n            # Add batch dimension to attention_mask if provided\n            if attention_mask is not None:\n                attention_mask = attention_mask.unsqueeze(0)\n        # Call original forward method with corrected inputs\n        return original_text_forward(self, input_ids=input_ids, attention_mask=attention_mask,\n                                      position_ids=position_ids, head_mask=head_mask,\n                                      inputs_embeds=inputs_embeds, encoder_embeds=encoder_embeds,\n                                      encoder_hidden_states=encoder_hidden_states,\n                                      encoder_attention_mask=encoder_attention_mask,\n                                      past_key_values=past_key_values, use_cache=use_cache,\n                                      output_attentions=output_attentions,\n                                      output_hidden_states=output_hidden_states,\n                                      return_dict=return_dict, is_decoder=is_decoder)\n    \n    # Store original forward method of BlipTextLMHeadModel for patching\n    original_lm_forward = BlipTextLMHeadModel.forward\n    def patched_lm_forward(self, input_ids=None, attention_mask=None, position_ids=None, \n                           head_mask=None, inputs_embeds=None, encoder_hidden_states=None, \n                           encoder_attention_mask=None, labels=None, past_key_values=None,\n                           use_cache=None, output_attentions=None, output_hidden_states=None,\n                           return_dict=None, return_logits=False, is_decoder=True, reduction=\"mean\"):\n        # Ensure input_ids has batch dimension for text encoding\n        if input_ids is not None and len(input_ids.shape) == 1:\n            input_ids = input_ids.unsqueeze(0)\n            # Add batch dimension to attention_mask if provided\n            if attention_mask is not None:\n                attention_mask = attention_mask.unsqueeze(0)\n        # Ensure labels has batch dimension for training\n        if labels is not None and len(labels.shape) == 1:\n            labels = labels.unsqueeze(0)\n        # Call original forward method with corrected inputs and labels\n        return original_lm_forward(self, input_ids=input_ids, attention_mask=attention_mask,\n                                    position_ids=position_ids, head_mask=head_mask,\n                                    inputs_embeds=inputs_embeds, encoder_hidden_states=encoder_hidden_states,\n                                    encoder_attention_mask=encoder_attention_mask, labels=labels,\n                                    past_key_values=past_key_values, use_cache=use_cache,\n                                    output_attentions=output_attentions, output_hidden_states=output_hidden_states,\n                                    return_dict=return_dict, return_logits=return_logits,\n                                    is_decoder=is_decoder, reduction=reduction)\n    \n    # Replace original forward methods with patched versions\n    BlipTextModel.forward = patched_text_forward\n    BlipTextLMHeadModel.forward = patched_lm_forward\n\n# Apply the patches to BLIP models for compatibility with single-sample inputs\npatch_blip_models()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:46:33.260980Z","iopub.execute_input":"2025-05-18T09:46:33.261279Z","iopub.status.idle":"2025-05-18T09:46:33.269760Z","shell.execute_reply.started":"2025-05-18T09:46:33.261246Z","shell.execute_reply":"2025-05-18T09:46:33.269036Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Custom Forward for LoRA Integration**\n* Customizes the model's forward method for LoRA compatibility","metadata":{}},{"cell_type":"code","source":"def create_custom_forward(model):\n    original_forward = model.base_model.forward\n    def custom_forward(self, input_ids=None, attention_mask=None, pixel_values=None,\n                       decoder_input_ids=None, decoder_attention_mask=None, output_attentions=None,\n                       output_hidden_states=None, labels=None, return_dict=None, \n                       interpolate_pos_encoding=None, **kwargs):\n        # Add batch dimension if needed\n        if input_ids is not None and len(input_ids.shape) == 1:\n            input_ids = input_ids.unsqueeze(0)\n            if attention_mask is not None:\n                attention_mask = attention_mask.unsqueeze(0)\n        # Filter allowed inputs\n        allowed_inputs = {\n            \"input_ids\": input_ids, \"attention_mask\": attention_mask, \"pixel_values\": pixel_values,\n            \"decoder_input_ids\": decoder_input_ids, \"decoder_attention_mask\": decoder_attention_mask,\n            \"output_attentions\": output_attentions, \"output_hidden_states\": output_hidden_states,\n            \"labels\": labels, \"return_dict\": return_dict, \"interpolate_pos_encoding\": interpolate_pos_encoding\n        }\n        filtered_inputs = {k: v for k, v in allowed_inputs.items() if v is not None}\n        return original_forward(**filtered_inputs)\n    # Assign custom forward to model\n    model.base_model.forward = types.MethodType(custom_forward, model.base_model)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:46:36.552458Z","iopub.execute_input":"2025-05-18T09:46:36.552710Z","iopub.status.idle":"2025-05-18T09:46:36.560296Z","shell.execute_reply.started":"2025-05-18T09:46:36.552692Z","shell.execute_reply":"2025-05-18T09:46:36.559399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model and Processor Setup**","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"Salesforce/blip-vqa-base\"\n\n# Load BLIP processor and model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(MODEL_NAME, device_map=\"auto\")\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\n        \"qkv\", \"projection\", \"self.query\", \"self.key\", \"self.value\"\n    ],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"QUESTION_ANS\"\n)\nmodel = get_peft_model(model, lora_config)\nmodel = create_custom_forward(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training Parameters and W&B Initialization**","metadata":{}},{"cell_type":"code","source":"# Define training parameters\nDATA_CSV = \"/kaggle/input/vrproject2dataset/vqa_dataset.csv\"\nIMAGE_DIR = \"/kaggle/input/vrproject2/abo-images-small/images/small\"\nBATCH_SIZE = 4  # Reduced for memory efficiency\nNUM_EPOCHS = 5\nLEARNING_RATE = 5e-5\n\n# Initialize Weights & Biases for logging\nwandb.init(project=\"blip-vqa-lora\", name=\"blip-vqa-run\", config={\n    \"model_name\": MODEL_NAME, \"batch_size\": BATCH_SIZE, \"num_epochs\": NUM_EPOCHS,\n    \"learning_rate\": LEARNING_RATE, \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.1\n}, mode=\"offline\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Dataset Loading and Splitting**","metadata":{}},{"cell_type":"code","source":"train_split_ratio = 0.9\n# Load and split dataset\nfull_df = pd.read_csv(DATA_CSV)\ntrain_split = int(train_split_ratio * len(full_df))\ndf = full_df[:train_split].reset_index(drop=True)  # 90% for training/validation\ntrain_df = df.sample(frac=0.9, random_state=42).reset_index(drop=True)  # 90% of df for training\nval_df = df.drop(train_df.index).reset_index(drop=True)  # Remaining 10% for validation\n\nprint(train_split)\nSUBSET_SIZE = train_split\n\n# Calculate training steps\ngradient_accumulation_steps = 8 #Number of batches to accumulate gradients over before updating the model (used to simulate a larger batch size).\neffective_batch_size = BATCH_SIZE * gradient_accumulation_steps\ntotal_train_samples = int(SUBSET_SIZE * train_split_ratio)\nmax_steps = (total_train_samples // effective_batch_size) * NUM_EPOCHS\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:47:21.090151Z","iopub.execute_input":"2025-05-18T09:47:21.090426Z","iopub.status.idle":"2025-05-18T09:47:21.334675Z","shell.execute_reply.started":"2025-05-18T09:47:21.090407Z","shell.execute_reply":"2025-05-18T09:47:21.334053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**VQA Dataset Generator**\n* **Purpose:** The class processes each sample on-the-fly, converting images and text into the format required by the model (e.g., tokenized text and normalized images)","metadata":{}},{"cell_type":"code","source":"class VQADatasetGenerator(IterableDataset):\n    def __init__(self, df, image_dir, processor):\n        self.df = df  # Pandas DataFrame with VQA data (columns: path, question, answer)\n        self.image_dir = image_dir  # Directory containing images\n        self.processor = processor  # BlipProcessor for text and image preprocessing\n\n    def __iter__(self):\n        for _, row in self.df.iterrows():  # Iterate over DataFrame rows\n            # Load image\n            img_path = os.path.join(self.image_dir, row[\"path\"])\n            image = Image.open(img_path).convert(\"RGB\")  # Load and convert to RGB\n            \n            # Prepare question and answer\n            question = f\"Question: {row['question']} Answer:\"  # Format question\n            answer = str(row['answer']).strip().lower()  # Clean answer\n            answer = re.sub(r'[^\\w\\s]', '', answer)  # Remove punctuation\n\n            # Process inputs\n            enc = self.processor(\n                images=image,\n                text=question,\n                return_tensors=\"pt\",  # Return PyTorch tensors\n                padding=\"max_length\",  # Pad to max length\n                truncation=True,  # Truncate if too long\n                max_length=32  # Max length for question\n            )\n            \n            # Process answer for decoder\n            decoder_tokens = self.processor.tokenizer(\n                answer,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=10,  # Max length for answer\n                return_tensors=\"pt\"\n            )\n            decoder_input_ids = decoder_tokens.input_ids.squeeze(0)  # Remove batch dimension\n\n            # Yield processed example\n            yield {\n                \"input_ids\": enc.input_ids.squeeze(0),  # Token IDs for question\n                \"attention_mask\": enc.attention_mask.squeeze(0),  # Attention mask for question\n                \"pixel_values\": enc.pixel_values.squeeze(0),  # Processed image\n                \"decoder_input_ids\": decoder_input_ids,  # Token IDs for answer\n                \"labels\": decoder_input_ids  # Same as decoder_input_ids for training\n            }\n\nprint(max_steps)  # Print total training steps calculated in Cell 6","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training Arguments**","metadata":{}},{"cell_type":"code","source":"# Configure training hyperparameters and settings for the Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"./blip-vqa-output\",\n    per_device_train_batch_size=BATCH_SIZE, # Number of samples per batch for training (per GPU)\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=8,  # Accumulate gradients over 8 batches before updating parameters\n    num_train_epochs=NUM_EPOCHS,\n    learning_rate=LEARNING_RATE, #Learning rate for optimizer\n    fp16=False,  # Mixed precision disabled\n    logging_steps=100, # Log training metrics every 100 steps\n    eval_steps=200, # Evaluate model every 200 steps\n    save_steps=1500,  #Save model checkpoint every 1500 steps\n    save_total_limit=10,\n    remove_unused_columns=False,\n    label_names=[\"labels\"],\n    report_to=\"wandb\", # Log metrics to Weights & Biases\n    run_name=\"blip-vqa-run\", # Name of the training run for W&B tracking\n    lr_scheduler_type=\"linear\", # Use a linear learning rate decay schedule\n    warmup_steps=100,\n    max_steps=max_steps # Total number of training steps (overrides epochs if set)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:47:30.353327Z","iopub.execute_input":"2025-05-18T09:47:30.353974Z","iopub.status.idle":"2025-05-18T09:47:30.383381Z","shell.execute_reply.started":"2025-05-18T09:47:30.353941Z","shell.execute_reply":"2025-05-18T09:47:30.382831Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Compute Metrics**\n* Evaluates model performance during training","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    # Decode predictions and labels\n    generated_ids, labels = eval_pred\n    pred_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n    label_texts = processor.batch_decode(labels, skip_special_tokens=True)\n    pred_texts = [re.sub(r'[^\\w\\s]', '', p.strip().lower()) for p in pred_texts]\n    label_texts = [re.sub(r'[^\\w\\s]', '', l.strip().lower()) for l in label_texts]\n    # Calculate metrics\n    acc = sum(1 for p, l in zip(pred_texts, label_texts) if p == l) / len(pred_texts)\n    f1 = f1_score(label_texts, pred_texts, average=\"macro\", zero_division=0)\n    wandb.log({\"eval_accuracy\": acc, \"eval_f1\": f1})\n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T09:47:33.356108Z","iopub.execute_input":"2025-05-18T09:47:33.356725Z","iopub.status.idle":"2025-05-18T09:47:33.362554Z","shell.execute_reply.started":"2025-05-18T09:47:33.356705Z","shell.execute_reply":"2025-05-18T09:47:33.361889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Trainer Setup and Training**","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=VQADatasetGenerator(train_df, IMAGE_DIR, processor), #converts train_df to format expected by the model\n    eval_dataset=VQADatasetGenerator(val_df, IMAGE_DIR, processor),\n    data_collator=default_data_collator, # combines multiple points from train_dataset to form batches \n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\n# Save trained model and processor\nmodel.save_pretrained(\"blip-vqa-adapters\")\nprocessor.save_pretrained(\"blip-vqa-adapters\")\nprint(\"Training complete!\")\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Inference Setup**","metadata":{}},{"cell_type":"code","source":"SUBSET = 5000\nMODEL_DIR = \"blip-vqa-adapters\"\nprocessor = BlipProcessor.from_pretrained('Salesforce/blip-vqa-base')\nbase_model = BlipForQuestionAnswering.from_pretrained(MODEL_NAME, device_map=\"auto\")\nmodel = PeftModel.from_pretrained(base_model, MODEL_DIR).to(DEVICE)\nmodel.eval()\n\n# # Setup BERTScore model\n# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# bert_model = BertModel.from_pretrained('bert-base-uncased').to(DEVICE)\n# bert_model.eval()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-18T09:48:26.542Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Metric Calculation Functions**","metadata":{}},{"cell_type":"code","source":"# Map NLTK POS tags to WordNet POS categories for WUP similarity\ndef get_wordnet_pos(word):\n    # Get POS tag for the word using NLTK\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    # Define mapping from NLTK tags to WordNet POS categories\n    tag_dict = {\"J\": wn.ADJ, \"N\": wn.NOUN, \"V\": wn.VERB, \"R\": wn.ADV}\n    # Return WordNet POS or default to NOUN if tag is unmapped\n    return tag_dict.get(tag, wn.NOUN)\n\n# Calculate Wu-Palmer (WUP) similarity for lexical similarity between predictions and references\ndef calculate_wup_score(preds, refs):\n    # Define helper function to compute WUP similarity for a single pred-ref pair\n    def wup_sim(pred, ref):\n        # Clean and tokenize prediction and reference text\n        pred_tokens = re.sub(r'[^\\w\\s]', '', pred.lower()).split()\n        ref_tokens = re.sub(r'[^\\w\\s]', '', ref.lower()).split()\n        # Return 0 if either token list is empty\n        if not pred_tokens or not ref_tokens:\n            return 0.0\n        # Initialize list to store maximum similarities for prediction tokens\n        max_similarities = []\n        # Iterate over each prediction token\n        for p_token in pred_tokens:\n            token_max_sim = 0.0\n            # Get synsets for prediction token with POS or fallback to all synsets\n            p_synsets = wn.synsets(p_token, pos=get_wordnet_pos(p_token)) or wn.synsets(p_token)\n            if not p_synsets:\n                continue\n            # Iterate over each reference token\n            for r_token in ref_tokens:\n                # Get synsets for reference token with POS or fallback\n                r_synsets = wn.synsets(r_token, pos=get_wordnet_pos(r_token)) or wn.synsets(r_token)\n                if not r_synsets:\n                    continue\n                # Compute WUP similarity for all synset pairs\n                token_sims = [wn.wup_similarity(p_syn, r_syn) or 0.0 for p_syn in p_synsets for r_syn in r_synsets]\n                if token_sims:\n                    # Store maximum similarity for this token pair\n                    token_max_sim = max(token_sims)\n            if token_max_sim > 0:\n                # Store non-zero maximum similarity\n                max_similarities.append(token_max_sim)\n        # Return average similarity or 0 if no valid similarities\n        return sum(max_similarities) / len(max_similarities) if max_similarities else 0.0\n    # Compute WUP similarity for all pred-ref pairs\n    wup_scores = [wup_sim(p, r) for p, r in zip(preds, refs)]\n    # Return average WUP score across all pairs\n    return sum(wup_scores) / len(wup_scores) if wup_scores else 0.0","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-18T09:48:26.542Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Test Data Preparation and Inference**","metadata":{}},{"cell_type":"code","source":"# Prepare test dataset by sampling from the remaining data after train/validation split\ntest_df = full_df[train_split:].sample(n=SUBSET, random_state=42).reset_index(drop=True)\n\n# Initialize lists to store predictions and ground-truth references\npreds, refs = [], []\n# Iterate over test data in batches for inference\nfor i in tqdm(range(0, len(test_df), BATCH_SIZE), desc=\"Inference\"):\n    batch = test_df.iloc[i:i + BATCH_SIZE]\n    \n    # Load and convert images to RGB format for the batch\n    images = [Image.open(os.path.join(IMAGE_DIR, p)).convert(\"RGB\") for p in batch[\"path\"]]\n    \n    # Format questions with standard prefix/suffix for model input\n    questions = [f\"Question: {q} Answer:\" for q in batch[\"question\"].tolist()]\n    # Process images and questions using BlipProcessor for model-compatible tensors\n    inputs = processor(images=images, text=questions, return_tensors=\"pt\", padding=True).to(DEVICE)\n    \n    with torch.no_grad():\n        # Generate answers using beam search for better quality\n        generated_ids = model.generate(**inputs, max_new_tokens=10, num_beams=5, early_stopping=True)\n    # Decode generated token IDs\n    raw = processor.batch_decode(generated_ids, skip_special_tokens=True)\n    \n    # Clean predictions and ground-truth answers by removing punctuation, converting to lowercase, and trimming\n    cleaned_p = [re.sub(r'[^\\w\\s]', '', r.strip().lower()) for r in raw]\n    cleaned_r = [re.sub(r'[^\\w\\s]', '', str(r).strip().lower()) for r in batch[\"answer\"]]\n    \n    preds.extend(cleaned_p)\n    refs.extend(cleaned_r)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-18T09:48:26.542Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Calculate Metrics**","metadata":{}},{"cell_type":"code","source":"acc = accuracy_score(refs, preds)\nprint(f\"String-match Accuracy: {acc*100:.2f}%\")\n\nP, R, F1 = bert_score(preds, refs, lang='en', rescale_with_baseline=True, device=DEVICE)\nprint(f\"BERTScore Precision: {P.mean().item()*100:.2f}%\")\nprint(f\"BERTScore Recall   : {R.mean().item()*100:.2f}%\")\nprint(f\"BERTScore F1       : {F1.mean().item()*100:.2f}%\")\n\nf1 = f1_score(refs, preds, average=\"macro\")\nprint(f\" â€¢ Macro F1  : {f1 * 100:.2f}%\")\n\nwup_score = calculate_wup_score(preds, refs)\nprint(f\"WUP Score: {wup_score*100:.2f}%\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-18T09:48:26.542Z"}},"outputs":[],"execution_count":null}]}